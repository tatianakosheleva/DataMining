{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ 4. –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–≤–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤—ã–π —ç—Ç–∞–ø - —Å–±–æ—Ä —Ç–≤–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–∫–ª—é—á–∞—Ç—å—Å—è –∫ Twitter API –∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å —Ç–≤–∏—Ç—ã –ø–æ id –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. \n",
    "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ API –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—Å–∞–Ω–æ –≤ –î–ó 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "CONSUMER_KEY = \"03Ai0vQvQ5cbkf2PqMLDKqhmE\"\n",
    "CONSUMER_SECRET = \"ROGs6elMuhmnfTuky25EhoNvvTbRT9uXe7c4r2GEw6LRQpxopE\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"4310324793-f8UuXkQui6m0bpwlt9Us3O8LQQMUVebBF8KCs6F\"\n",
    "ACCESS_TOKEN_SECRET = \"oySIWDx2F55bTyVNEqvhi5Qmc2ZQUlKtxVrwthB9IQrkQ\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET,\n",
    "                  sleep_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–≤–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–µ—Ç–æ–¥ GetUserTimeline –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ python-twitter. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –Ω–µ –±–æ–ª–µ–µ 200 —Ç–≤–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ü–æ –∫–∞–∂–¥–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±—Ä–∞—Ç—å 200 —Ç–≤–∏—Ç–æ–≤.\n",
    "\n",
    "–ú–µ—Ç–æ–¥ –∏–º–µ–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ –≤—Ä–µ–º–µ–Ω–∏, –∫–æ—Ç–æ—Ä–æ–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–æ–∂–¥–∞—Ç—å –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ API –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–µ—Ç–æ–¥ `GetSleepTime`. –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `GetUserTimeLine` –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–∑—ã–≤–∞—Ç—å `GetSleepTime` —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º \"statuses/user_timeline\".\n",
    "\n",
    "–ú–µ—Ç–æ–¥ GetUserTimeline –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã —Ç–∏–ø–∞ Status. –£ —ç—Ç–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –µ—Å—Ç—å –º–µ—Ç–æ–¥ AsDict, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å —Ç–≤–∏—Ç –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è.\n",
    "\n",
    "Id –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—á–∏—Ç–∞—Ç—å –∏–∑ —Ñ–∞–π–ª–∞, –∫–∞–∫ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ –≤ –î–ó 1.\n",
    "\n",
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_user_tweets(user_id)`. –í—Ö–æ–¥–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - id –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ —Ñ–∞–π–ª–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –º–∞—Å—Å–∏–≤ —Ç–≤–∏—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –≥–¥–µ –∫–∞–∂–¥—ã–π —Ç–≤–∏—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ —Ç–≤–∏—Ç–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–ø–∏—Å–∞–ª —Å–∞–º. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –¥—Ä—É–≥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º, —Å—Å—ã–ª–∫–∏ –∏ —Ä–µ—Ç–≤–∏—Ç—ã, –∞ —Ç–∞–∫ –∂–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –≤–∏–¥–µ–æ, —Ç–∞–∫ –∫–∞–∫ –Ω–∞—à–∞ —Ü–µ–ª—å - –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\"\"\"import os\n",
    "import json\n",
    "file_name = 'downloded_tweets'\n",
    "if not os.path.exists(file_name):\n",
    "    os.makedirs(file_name)\"\"\"\n",
    "    \n",
    "    \n",
    "def get_user_tweets(user_id):\n",
    "    \"\"\"returns list of tweets as dicts\"\"\"\n",
    "    # your code here\n",
    "    #global file_name\n",
    "    user_tweets = []\n",
    "    try:\n",
    "        statuses = api.GetUserTimeline(user_id=user_id, count=200, include_rts ='false', exclude_replies ='true') # exclude retweets and replies\n",
    "    except twitter.TwitterError:\n",
    "        print 'error'\n",
    "        statuses = ''\n",
    "    except ValueError:\n",
    "        print 'error'\n",
    "        statuses = ''\n",
    "    for s in statuses:\n",
    "        if (not s.urls) and (s.media is None): # do not include url and media (photo and video)\n",
    "            s.text = re.sub(r\"http\\S+\", \"\", s.text) # if there is other links in tweets\n",
    "            user_tweets.append({'lang': s.lang, \n",
    "                 'favorited': s.favorited, \n",
    "                 'truncated': s.truncated, \n",
    "                 'text': s.text, \n",
    "                 'created_at': s.created_at, \n",
    "                 'retweeted': s.retweeted, \n",
    "                 'source': s.source, \n",
    "                 'user': {'id': s.user.id}, \n",
    "                 'id': s.id})\n",
    "    return user_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#get_user_tweets(2330149164)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–∞–∑–±–æ—Ä —Ç–µ–∫—Å—Ç–∞ —Ç–≤–∏—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã - –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–ª–æ–≤–∞. –ú—ã –±—É–¥–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞ –∫ —Å–ª–æ–≤–∞–º. –î–ª—è —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —Å–ª–æ–≤–∞. –°–¥–µ–ª–∞—Ç—å —ç—Ç–æ –º–æ–∂–Ω–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.\n",
    "\n",
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, `get_words(text)`. –í—Ö–æ–¥–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - —Å—Ç—Ä–æ–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º. –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ (—Å–ª–æ–≤). –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –Ω—É–∂–Ω–æ —É—á–µ—Å—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ–µ –Ω–∞–ª–∏—á–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –≤—ã–¥–µ–ª–∏—Ç—å –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    # your code here\n",
    "    text = text.lower()\n",
    "    words = []\n",
    "    for w in text.split():\n",
    "        w = re.sub(r'[!\\?@.,;:\\-%\\#\\(\\)\\$\\\\/\\'\\\"\\*\\+\\[\\]<>_&\\d]', '', w) # delete symbols\n",
    "        if w: # if w is not empty string\n",
    "            words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get_words(u'üòè ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ. –¢–æ –µ—Å—Ç—å –ø—Ä–∏–≤–µ—Å—Ç–∏ –∏—Ö –∫ —Ñ–æ—Ä–º–µ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø—Ä. –°–¥–µ–ª–∞—Ç—å —ç—Ç–æ –º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ nltk. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–≥—Ä—É–∑–∫–µ, —É—Å—Ç–∞–Ω–æ–≤–∫–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç–µ http://www.nltk.org/\n",
    "\n",
    "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π –º–æ–∂–Ω–æ –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–µ—Ç–æ–¥–æ–º download –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É. \n",
    "\n",
    "–î–ª—è –ø—Ä–∏–≤–µ–¥–µ–Ω–∏—è –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `WordNetLemmatizer` –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ nltk. –£ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –µ—Å—Ç—å –º–µ—Ç–æ–¥ `lemmatize`.\n",
    "\n",
    "–¢–∞–∫–∂–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–±—Ä–∞—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞. –≠—Ç–æ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å–ª–æ–≤–∞, –Ω–µ –Ω–µ—Å—É—â–∏–µ —Å–º—ã—Å–ª–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –¥–ª—è –Ω–∞—à–∏—Ö –∑–∞–¥–∞—á. –°–¥–µ–ª–∞—Ç—å —ç—Ç–æ –º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é `stopwords` –∏–∑ nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_tokens(words)`. –í—Ö–æ–¥–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - –º–∞—Å—Å–∏–≤ —Å–ª–æ–≤. –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –º–∞—Å—Å–∏–≤ —Ç–æ–∫–µ–Ω–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def get_tokens(words):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    # your code here\n",
    "    tokens = []\n",
    "    stop_words = stopwords.words('english') # get stop words\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for w in words:\n",
    "        w = wnl.lemmatize(w) # normalize\n",
    "        if (w not in stop_words) and w.isalpha(): # exclude smiles\n",
    "            tokens.append(w)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `get_tweet_tokens(tweet)`. –í—Ö–æ–¥–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä - —Ç–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ -- —Ç–æ–∫–µ–Ω—ã —Ç–≤–∏—Ç–∞. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    # your code here\n",
    "    words = get_words(tweet)\n",
    "    return get_tokens(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `collect_users_tokens()`. –§—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ —Å–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π. –í —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü–µ —Å—Ç—Ä–æ–∫–∞ - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å. –°—Ç–æ–ª–±–µ—Ü - —Ç–æ–∫–µ–Ω. –ù–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–∏ - —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ç–æ–∫–µ–Ω –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `DictVectorizer` –∏–∑ `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_tokens(user_tokens): # create dictionary to count user tokens\n",
    "    tokens_amt = {}\n",
    "    for token in user_tokens:\n",
    "        if token in tokens_amt:\n",
    "            tokens_amt[token] += 1\n",
    "        else:\n",
    "            tokens_amt[token] = 1\n",
    "    return tokens_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collect_users_tokens(df_users):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    # your code here\n",
    "    # save intermediate result\n",
    "    users = df_users['uid'].values.tolist()\n",
    "    user_tokens = [] # list of user tokens\n",
    "    user_tokens_amt = [] # list of dictionaries\n",
    "    \n",
    "    for user in users:\n",
    "        #print user\n",
    "        if users.index(user) & 127 == 0:\n",
    "            print users.index(user)\n",
    "        user_tweets = get_user_tweets(user) # get user tweets\n",
    "        for tweet in user_tweets:\n",
    "            user_tokens += get_tweet_tokens(tweet['text'])\n",
    "        user_tokens_amt.append(count_tokens(user_tokens))\n",
    "         \n",
    "    return users, user_tokens_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "error\n",
      "128\n",
      "error\n",
      "error\n",
      "256\n",
      "error\n",
      "384\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "512\n",
      "error\n",
      "error\n",
      "error\n",
      "640\n",
      "error\n",
      "error\n",
      "768\n",
      "error\n",
      "error\n",
      "error\n",
      "896\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "1024\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "1152\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "1280\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "1408\n",
      "error\n",
      "error\n",
      "error\n",
      "1536\n",
      "error\n",
      "error\n",
      "error\n",
      "1664\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\"\"\"v = DictVectorizer()\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "print v.fit_transform(D).toarray()\n",
    "print v.get_feature_names()\"\"\"\n",
    "\n",
    "TRAINING_SET_URL = \"train.csv\"\n",
    "EXAMPLE_SET_URL = \"test.csv\"\n",
    "df_users_train = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=0)\n",
    "df_users_ex = pd.read_csv(EXAMPLE_SET_URL, sep=\",\", header=0)\n",
    "df_users_ex['cls'] = None\n",
    "df_users = pd.concat([df_users_train, df_users_ex]) # –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω–Ω—Ç–∞—Ä–∏–π –ø–µ—Ä–µ–¥ —Å—Ç—Ä–æ—á–∫–æ–π!\n",
    "#df_users = df_users[3072:3200]\n",
    "\n",
    "\n",
    "users, users_tokens = collect_users_tokens(df_users) # –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω–Ω—Ç–∞—Ä–∏–π –ø–µ—Ä–µ–¥ —Å—Ç—Ä–æ—á–∫–æ–π!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_users3000 = df_users[:3000]\n",
    "users3000, users_tokens3000 = collect_users_tokens(df_users3000)\n",
    "print 'ok'\n",
    "df_users6000 = df_users[3000:6000]\n",
    "users6000, users_tokens6000 = collect_users_tokens(df_users6000)\n",
    "print 'ok'\n",
    "df_users9000 = df_users[6000:9000]\n",
    "users9000, users_tokens9000 = collect_users_tokens(df_users9000)\n",
    "print 'ok'\n",
    "df_users11000 = df_users[9000:]\n",
    "users11000, users_tokens11000 = collect_users_tokens(df_users11000)\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–∏–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ savez –∏–∑ numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez(\"files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –æ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –µ–µ –≤ –≤–∏–¥–µ –æ–±–ª–∞–∫–∞ —Ç—ç–≥–æ–≤. [–ü–æ–¥—Å–∫–∞–∑–∫–∞](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_tag_cloud(v, vs):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    # your code here\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
